{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "UkY5vWEe6lWh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## REINFORCE"
      ],
      "metadata": {
        "id": "Pfg7l4d_6VXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import sumo_rl\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, output_size)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        return self.softmax(x)\n",
        "\n",
        "# Hyperparams\n",
        "learning_rate = 0.01\n",
        "gamma = 0.99\n",
        "num_episodes = 500\n",
        "NUM_BINS = 10\n",
        "\n",
        "env = gym.make('sumo-rl-v0',\n",
        "               net_file='nets/single-intersection/single-intersection.net.xml',\n",
        "               route_file='nets/single-intersection/single-intersection.rou.xml',\n",
        "               use_gui=False,\n",
        "               num_seconds=1000)\n",
        "\n",
        "\n",
        "#dimensions for neural net based on state and action space\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "policy_network = PolicyNetwork(state_size, action_size)\n",
        "optimizer = optim.Adam(policy_network.parameters(), lr=learning_rate)\n",
        "\n",
        "episode_rewards = []\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state, info = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    log_probs = []\n",
        "    rewards = []\n",
        "\n",
        "    while not done:\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "\n",
        "        action_probs = policy_network(state_tensor)\n",
        "        action = np.random.choice(action_size, p=action_probs.detach().numpy().flatten())\n",
        "\n",
        "        log_probs.append(torch.log(action_probs[0, action]))\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        rewards.append(reward)\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if len(rewards) >= 999:\n",
        "            done = True\n",
        "\n",
        "    episode_rewards.append(total_reward)\n",
        "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward}\")\n",
        "\n",
        "    discounted_rewards = []\n",
        "    cumulative_reward = 0\n",
        "    for r in reversed(rewards):\n",
        "        cumulative_reward = r + gamma * cumulative_reward\n",
        "        discounted_rewards.insert(0, cumulative_reward)\n",
        "\n",
        "    discounted_rewards = torch.FloatTensor(discounted_rewards)\n",
        "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
        "\n",
        "    policy_loss = [-log_prob * reward for log_prob, reward in zip(log_probs, discounted_rewards)]\n",
        "    policy_loss = torch.stack(policy_loss).sum()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "def moving_average(data, window_size):\n",
        "    cumsum = np.cumsum(data)\n",
        "    return (cumsum[window_size:] - cumsum[:-window_size]) / window_size\n",
        "\n",
        "window_size = 10\n",
        "smoothed_rewards = moving_average(episode_rewards, window_size)\n",
        "\n",
        "\n",
        "plt.plot(range(window_size, num_episodes), smoothed_rewards)\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Smoothed Cumulative Reward')\n",
        "plt.title(f'Smoothed Cumulative Reward per Episode (Moving Avg Window: {window_size})')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "iBOgB8bv6UyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Round Robin (time-based trigger)"
      ],
      "metadata": {
        "id": "Tby7C7z26rNb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74B4Wt4J6TXx"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import sumo_rl\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "env = gym.make('sumo-rl-v0',\n",
        "               net_file='nets/single-intersection/single-intersection.net.xml',\n",
        "               route_file='nets/single-intersection/single-intersection.rou.xml',\n",
        "               out_csv_name='path_to_output.csv',\n",
        "               use_gui=False,\n",
        "               num_seconds=1000)\n",
        "\n",
        "# Tracking rewards over episodes\n",
        "episode_rewards = []\n",
        "num_episodes = 500  # Define the number of episodes for training\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    obs, info = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    prev_phase_one_hot = 0\n",
        "\n",
        "    while not done:\n",
        "        phase_one_hot, min_green = obs[0], obs[1]\n",
        "\n",
        "        if min_green and prev_phase_one_hot != phase_one_hot:\n",
        "            action = 1 if phase_one_hot == 1 else 0\n",
        "        else:\n",
        "            action = phase_one_hot\n",
        "\n",
        "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "        obs = next_obs\n",
        "        prev_phase_one_hot = phase_one_hot\n",
        "        total_reward += reward\n",
        "        done = terminated or truncated\n",
        "\n",
        "    episode_rewards.append(total_reward)\n",
        "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward}\")\n",
        "\n",
        "# Function to calculate moving average\n",
        "def moving_average(data, window_size):\n",
        "    cumsum = np.cumsum(data)\n",
        "    return (cumsum[window_size:] - cumsum[:-window_size]) / window_size\n",
        "\n",
        "# Plotting cumulative rewards over episodes with smoothing\n",
        "window_size = 10  # Adjust the window size for smoothing\n",
        "smoothed_rewards = moving_average(episode_rewards, window_size)\n",
        "\n",
        "plt.plot(range(window_size, num_episodes), smoothed_rewards)\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Smoothed Cumulative Reward')\n",
        "plt.title(f'Smoothed Cumulative Reward per Episode (Moving Avg Window: {window_size})')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Round Robin (traffic density-based trigger)"
      ],
      "metadata": {
        "id": "aYaoM1Wn6wVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import sumo_rl\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "env = gym.make('sumo-rl-v0',\n",
        "               net_file='nets/single-intersection/single-intersection.net.xml',\n",
        "               route_file='nets/single-intersection/single-intersection.rou.xml',\n",
        "               out_csv_name='path_to_output.csv',\n",
        "               use_gui=False,\n",
        "               num_seconds=1000)\n",
        "\n",
        "# Tracking rewards over episodes\n",
        "episode_rewards = []\n",
        "num_episodes = 500  # Define the number of episodes for training\n",
        "lane_density_threshold = 0.2 # Define the density threshold to switch light\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    obs, info = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    prev_phase_one_hot = 0\n",
        "\n",
        "    while not done:\n",
        "        phase_one_hot, lane_1_density, lane_2_density = obs[0], obs[2], obs[3]\n",
        "\n",
        "        if prev_phase_one_hot != phase_one_hot and (lane_1_density > lane_density_threshold or lane_2_density > lane_density_threshold):\n",
        "            action = 1 if phase_one_hot == 1 else 0\n",
        "        else:\n",
        "            action = phase_one_hot\n",
        "\n",
        "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "        obs = next_obs\n",
        "        prev_phase_one_hot = phase_one_hot\n",
        "        total_reward += reward\n",
        "        done = terminated or truncated\n",
        "\n",
        "    episode_rewards.append(total_reward)\n",
        "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward}\")\n",
        "\n",
        "# Function to calculate moving average\n",
        "def moving_average(data, window_size):\n",
        "    cumsum = np.cumsum(data)\n",
        "    return (cumsum[window_size:] - cumsum[:-window_size]) / window_size\n",
        "\n",
        "# Plotting cumulative rewards over episodes with smoothing\n",
        "window_size = 10  # Adjust the window size for smoothing\n",
        "smoothed_rewards = moving_average(episode_rewards, window_size)\n",
        "\n",
        "plt.plot(range(window_size, num_episodes), smoothed_rewards)\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Smoothed Cumulative Reward')\n",
        "plt.title(f'Smoothed Cumulative Reward per Episode (Moving Avg Window: {window_size})')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iLpS0WK06z_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-Learning"
      ],
      "metadata": {
        "id": "seLdwpTK61Kk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import sumo_rl\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define Q-learning parameters\n",
        "Q = {}  # Q-table\n",
        "alpha = 0.005  # Learning rate\n",
        "gamma = 0.9  # Discount factor\n",
        "epsilon = 1.0  # Initial epsilon for epsilon-greedy strategy\n",
        "min_epsilon = 0.01  # Minimum epsilon value\n",
        "epsilon_decay = 0.990  # Epsilon decay rate\n",
        "\n",
        "env = gym.make('sumo-rl-v0',\n",
        "               net_file='nets/single-intersection/single-intersection.net.xml',\n",
        "               route_file='nets/single-intersection/single-intersection.rou.xml',\n",
        "               out_csv_name='path_to_output.csv',\n",
        "               use_gui=False,\n",
        "               num_seconds=1000)\n",
        "\n",
        "# Tracking rewards over episodes\n",
        "episode_rewards = []\n",
        "num_episodes = 500  # Define the number of episodes for training\n",
        "NUM_BINS = 10  # Number of bins for discretization\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    obs, info = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        # Discretize continuous decimal numbers in the observation\n",
        "        discretized_values = [int(value * NUM_BINS) for value in obs[3:]]\n",
        "        obs_tuple = tuple(list(obs[:3]) + discretized_values)\n",
        "\n",
        "        # Epsilon-greedy action selection\n",
        "        if np.random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            if obs_tuple in Q:\n",
        "                action = np.argmax(Q[obs_tuple])\n",
        "            else:\n",
        "                action = env.action_space.sample()\n",
        "\n",
        "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "        next_discretized_values = [int(val * NUM_BINS) for val in next_obs[3:]]\n",
        "        next_obs_tuple = tuple(list(next_obs[:3]) + next_discretized_values)\n",
        "\n",
        "        if obs_tuple not in Q:\n",
        "            Q[obs_tuple] = np.zeros(env.action_space.n)\n",
        "        if next_obs_tuple not in Q:\n",
        "            Q[next_obs_tuple] = np.zeros(env.action_space.n)\n",
        "\n",
        "        Q[obs_tuple][action] += alpha * (reward + gamma * max(Q[next_obs_tuple]) - Q[obs_tuple][action])\n",
        "\n",
        "        obs = next_obs\n",
        "        total_reward += reward\n",
        "        done = terminated or truncated\n",
        "\n",
        "    episode_rewards.append(total_reward)\n",
        "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward}\")\n",
        "\n",
        "    # Epsilon decay\n",
        "    if epsilon > min_epsilon:\n",
        "        epsilon *= epsilon_decay\n",
        "\n",
        "\n",
        "# Function to calculate moving average\n",
        "def moving_average(data, window_size):\n",
        "    cumsum = np.cumsum(data)\n",
        "    return (cumsum[window_size:] - cumsum[:-window_size]) / window_size\n",
        "\n",
        "# Plotting cumulative rewards over episodes with smoothing\n",
        "window_size = 10  # Adjust the window size for smoothing\n",
        "smoothed_rewards = moving_average(episode_rewards, window_size)\n",
        "\n",
        "plt.plot(range(window_size, num_episodes), smoothed_rewards)\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Smoothed Cumulative Reward')\n",
        "plt.title(f'Smoothed Cumulative Reward per Episode (Moving Avg Window: {window_size})')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OpSNJqik60lG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmarks"
      ],
      "metadata": {
        "id": "sK_5sdFr662Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IntelliLight\n",
        "\n",
        "https://github.com/wingsweihua/IntelliLight"
      ],
      "metadata": {
        "id": "vqM4Vo0l685Z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tndpSgVN6-qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fixed-time Control"
      ],
      "metadata": {
        "id": "dqAcCKn36-3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S23KSFO27Dvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actuated Control"
      ],
      "metadata": {
        "id": "QEAVeFqj7D74"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6_bHLr5d7GNO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}